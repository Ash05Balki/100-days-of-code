{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51881eac",
   "metadata": {},
   "source": [
    "# EDA & Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609faad",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403333bb",
   "metadata": {},
   "source": [
    "Data Used: https://www.kaggle.com/yeanzc/telco-customer-churn-ibm-dataset\n",
    "\n",
    "The dataset represents Telco data of customers, the services opted, their tenure, monthly charges etc., and a label indicating if the customer churned post a certain tenure.\n",
    "\n",
    "The Data also has other features describing the \"Churn Label\" like \"Churn Value\",\"Churn Score\",\"CLTV\" and \"Churn Reason\" which can be used for analysis, but not for predicting churn. \n",
    "\n",
    "You can take a classification approach having \"Churn Label\" as the dependent variable or a regression approach having \"Churn Score\" as the dependent variable.\n",
    "\n",
    "I am using the dataset not just to apply a predictive model, but to try out extensive hypothesis testing techniques.\n",
    "This simple dataset was chosen as it has both categorical and numerical independent variables and you have categorical and numerical dependent variables to consider.\n",
    "\n",
    "Let's start with basic visualisation and EDA to get a hold of the data and the distribution it follows before moving to sampling and hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab47ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages used\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf96b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data file as a dataframe\n",
    "df=pd.read_excel(\"Telco_customer_churn.xlsx\")\n",
    "pd.set_option('display.max_columns', None) #to display all the columns of the dataframe\n",
    "df.head() #viewing the first 5 rows by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_share=(df[['Churn Label']].value_counts()*100.0/len(df)).reset_index() # This indicates that the data set has imbalanced class labels\n",
    "list_tgt=list(target_share.columns)\n",
    "list_tgt[-1:]=['Share']\n",
    "target_share.columns =list_tgt\n",
    "target_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churned=df[df['Churn Label']=='Yes']\n",
    "df_notchurned=df[df['Churn Label']=='No']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d180daea",
   "metadata": {},
   "source": [
    "Creating a list of columns to be used as independent features. The list is referred back during EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of numerical and categorical column names by filtering on dtype\n",
    "df_dtypes=df.dtypes.reset_index()\n",
    "df_numeric=df_dtypes[df_dtypes[0]!='object']\n",
    "df_categoric=df_dtypes[df_dtypes[0]=='object']\n",
    "\n",
    "numeric_cols=list(df_numeric['index'])\n",
    "categoric_cols=list(df_categoric['index'])\n",
    "\n",
    "#Manually fixing mix up of numerical and categorical columns and removing the columns not used currently\n",
    "numeric_cols.append('Total Charges')\n",
    "numeric_cols.remove('Zip Code')\n",
    "numeric_cols.remove('Churn Value')\n",
    "numeric_cols.remove('CLTV')\n",
    "categoric_cols.append('Zip Code')\n",
    "categoric_cols.remove('Total Charges')\n",
    "categoric_cols.remove('Churn Reason')\n",
    "categoric_cols.remove('Lat Long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27284c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categoric_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62613b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deff2df",
   "metadata": {},
   "source": [
    "# Describe and Visualize Categorical variables \n",
    "### To understand their relationship with the Churn Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e93d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describing the categorical columns gives details on the unique values it has\n",
    "#and the frequency of the top occuring category within each of them.\n",
    "df[categoric_cols].astype('object').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391afdf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Columns with a constant value don't contribute to predicting different churn labels\n",
    "temp=df[categoric_cols].astype('object').describe().T\n",
    "temp[temp['unique']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba674e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp[temp['unique']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422cddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with a constant value don't contribute to predicting different churn labels\n",
    "#Columns with a large number (like n>100) of categorical values are hard to visualise\n",
    "categories_chk=list(temp[(temp['unique']>1) & (temp['unique']<5)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the data of churned and unchurned customers separately \n",
    "# can help us identify the column that differentiates for churn label with ease.\n",
    "\n",
    "# This being a preliminary check, you have to look for imbalance in categories due to natural pattern.\n",
    "# To begin with our target variable is imbalanced. 73.463% of users haven't churned as opposed to the other 26.537% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1e916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_churned_T=df_churned[categoric_cols].astype('object').describe().T\n",
    "df_churned_T['share']=df_churned_T['freq']*100.0/df_churned_T['count']\n",
    "df_churned_T.columns=[x+'_churned' for x in df_churned_T.columns]\n",
    "df_churned_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d4adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_notchurned_T=df_notchurned[categoric_cols].astype('object').describe().T\n",
    "df_notchurned_T['share']=df_notchurned_T['freq']*100.0/df_notchurned_T['count']\n",
    "df_notchurned_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88580b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat([df_notchurned_T[['top','share']], df_churned_T[['top_churned','share_churned']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd24d2",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "The above table can be used to find out distinguishing features that can explain the variability in predicting churn.\n",
    "\n",
    "The \"top\" column represents the most frequent value in a column (refer index here) and the share out of all unchurned  instances as \"share\" for unchurned customers.\n",
    "The \"top_churned\" column represents the most frequent value in a column (refer index here) and the share out of all churned instances as \"share_churned\" for churned customers.\n",
    "\n",
    "There are two steps involved in identifying the significant columns/features.\n",
    "1. Compare the top and top_churned columns, if they are different and if share_churned has notably different value the feature is contrasting well enough between churned and unchurned instances.\n",
    "Eg: Partner, Internet Service, Online Backup, Streaming TV, Streaming Movies, Payment Method\n",
    "2.  Compare the top and top_churned columns, if they are same and if share_churned has notably different value the feature is contrasting well enough between churned and unchurned instances.\n",
    "Eg: Senior Citizen, Dependents, Online Security, Device Protection,  Tech Support,, Contract, Paperless Billing\n",
    "3. Compare the top and top_churned columns, if they are different and if share_churned is indifferent the feature is contrasting well enough between churned and unchurned instances.\n",
    "Eg: Gender, Multiple Lines\n",
    "4. Compare the top and top_churned columns, if they are same and if share_churned is indifferent the feature is contrasting well enough between churned and unchurned instances.\n",
    "Eg: Phone Service\n",
    "\n",
    "Understanding a lot of variables and ones with more than 2 categorical values can be tricky using this descriptive method. Hence, let's look at how we can understand significant variables (the ones showing variability w.r.t. to target label) through visualisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752c631",
   "metadata": {},
   "source": [
    "### Let's visualize and understand the same from grouped bar graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e10931",
   "metadata": {},
   "source": [
    "Here, we are displaying grouped bar graphs to display the share of a particular categorical value in a column and how it is split within churn and not churned instances.\n",
    "\n",
    "eg: Consider the independent variable 'Gender' having values 'Female' and 'Male'. We plot the 'Female' instances that have churned (Churn Label -> Yes) as a % of all the female instances and vice versa for not chunred instances (Churn Label -> No).\n",
    "\n",
    "We compute the same share for all 'Male' instances and plot these side by side.\n",
    "\n",
    "In the above descriptive statistics we saw of all the churned instances (Churn Label -> Yes), what % are 'Female','Male' etc., displaying the which value is dominant in churned users within each categorical variable.\n",
    "\n",
    "Note that below we have a different view trying to show if a particular value within a categorical variable is skewed largely towards churn/ not.\n",
    "\n",
    "It's important to note that the skew might have a natural imbalance due to the 73-27 split of not churned and churned instances respectively in the target label. And any share value greater or lesser than this natural threshold contributes to a skew, which gives additional information to predict the target variable with aacuracy.\n",
    "\n",
    "This natural threshold of 73.463 for un-churned users and 26.537 for churned users are added to the graph as a horizontal line plot to visualise this better.\n",
    "\n",
    "From the above descriptive analysis we found that Gender is an insignificant feature as it has an equal mix of both 'Male' and 'Female' within each Churn Label.\n",
    "\n",
    "If you look at the visualisation as opposed to the inference from the above line, you can understand how the skew is represented for \"Gender\". This gives us a clear picture that the bar graph being approximately near the threshhold lines shows the natural skew in the Churn Label and not the variability.\n",
    "\n",
    "Similarly, other features like  Internet Service, Tech Support, Contract, Payment Method the share of few or more values are skewed towards one of the two target labels.\n",
    "\n",
    "Eg: Internet Service - Fiber Optic Users are skewed (bar is higher than the Churned-Ref threshold line) towards Churning (Churn Label -> Yes)\n",
    "\n",
    "This is how you can interpret categorical variables with 2-5 values with ease using grouped bar charts. This can be used for binary or multi-classification(preferably upto 5 class labels). The limitation of the values to 5 is just for ease of interpretability from these graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "for i in categories_chk:\n",
    "    if i !='Churn Label':\n",
    "        temp=pd.pivot_table(df, values = 'Count', index=[i,'Churn Label'],aggfunc='count').reset_index()\n",
    "\n",
    "        t=temp.groupby(i)['Count'].sum().reset_index()\n",
    "        final=pd.merge(temp,t,on=i,suffixes=('','_all'))\n",
    "        final['share']=final['Count']*100.0/final['Count_all']\n",
    "        print (final)\n",
    "        y_no=target_share[target_share['Churn Label']=='No']['Share']\n",
    "        y_yes=target_share[target_share['Churn Label']=='Yes']['Share']\n",
    "        fig = px.bar(final, x=\"Churn Label\", y=\"share\", color=i, barmode=\"group\", title=\"Churn - \"+i+\" Share\")\n",
    "\n",
    "        fig.add_hline(y=float(y_no),line_color=\"green\",annotation_text=\"Not Churned - Ref \"+str(round(float(y_no),3)),\n",
    "                      annotation_position=\"bottom right\")\n",
    "        fig.add_hline(y=float(y_yes),line_color=\"red\",annotation_text=\"Churned - Ref \"+str(round(float(y_yes),3)),\n",
    "                      annotation_position=\"top right\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06e027",
   "metadata": {},
   "source": [
    "Now that we've gone over how we can plot a bar graph and how we can interpret the same to identify important features in a small dataset. Let's move on to understand how the importance translates while modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbcb010",
   "metadata": {},
   "source": [
    "First let's understand the correlation between the categorical features.\n",
    "Here we are encoding the categorical values into numerical values using Label Encoder, since the categorical features are not ordinal.\n",
    "We then compute the correlation between every pair of features and plot then as a heatmap.\n",
    "You can hover over the heatmap to get the correlation score between any pair below.\n",
    "\n",
    "Multiple highly correlated features tend to decrease the performance of model, hence this is an important check.\n",
    "Also note that Correlation is not necesarily transitive always.\n",
    "\n",
    "Eg:  \n",
    "Online Security -> Partner and Partner -> Multiple Lines but Online Security is not correlated to Multiple Lines.\n",
    "https://terrytao.wordpress.com/2014/06/05/when-is-correlation-transitive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df_coded=df[categories_chk].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "corr = df_coded.corr()\n",
    "corr=round(corr,3)\n",
    "#sns.heatmap(corr)\n",
    "fig = px.imshow(corr,labels=dict(color=\"Correlation\"),)\n",
    "fig.update_xaxes(side=\"top\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16979e3e",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbfe78ed",
   "metadata": {},
   "source": [
    "This is how you might ideally go about it, but the correlations are in negative. This isn't possible in categorical features, since there is no inverse relation between categorical features. \n",
    "\n",
    "The correlation function uses pearson's correlation method for numerical data to calculate the same, which gives irrelevant results as above.\n",
    "\n",
    "The method for calculating correlation is different when it is between numeric, ordinal and nominal features.\n",
    "\n",
    "Since our data is nominal and also not all columns are dichotomous, we are using Cramers'V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    #print (np.sqrt(phi2corr/min((kcorr-1),(rcorr-1))))\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a20ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_list=list(df_coded.columns)\n",
    "len(corr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cat=corr\n",
    "for i in range(len(corr)):\n",
    "    for j in range(len(corr)):\n",
    "        corr_cat.iloc[i,j]=cramers_v(df_coded[corr_list[i]],df_coded[corr_list[j]])\n",
    "        \n",
    "corr_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee132c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "corr_val=corr_cat.values\n",
    "x_names=list(corr_cat.columns)\n",
    "y_names=list(corr_cat.index)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8a83619",
   "metadata": {},
   "source": [
    "#print(corr_val[:4,:4])\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "min_=-1\n",
    "max_=1\n",
    "norm_corr=round((corr-min_)/(max_-min_),1)\n",
    "data_=norm_corr.values[:4,:4]\n",
    "data_val=corr.values[:4,:4]\n",
    "data=data_.tolist()\n",
    "\n",
    "colorscale = [(0.0,'#228B22'),(0.1,'#228B22'), #green\n",
    "                 (0.11,'#87CEEB'),(0.3,'#87CEEB'), #blue\n",
    "                 (0.31,'#FFDAB9'),(0.4,'#FFDAB9'), #salmon\n",
    "                 (0.41,'#8B0000'),(0.6,'#8B0000'), #red\n",
    "                 (0.61,'#FFDAB9'),(0.7,'#FFDAB9'), #salmon\n",
    "                 (0.71,'#87CEEB'),(0.9,'#87CEEB'), #blue\n",
    "                 (0.91,'#228B22'),(1.0,'#228B22')] #red\n",
    "\n",
    "z = [[.1, .3, .9, 0.8],\n",
    "     [1, 0, .6, .7],\n",
    "     [.2, 0.5, 0, .7],\n",
    "     [.9, .8, .4, .2]]\n",
    "# print (z)\n",
    "# print (type(z[0][0]))\n",
    "# print(type(z))\n",
    "\n",
    "\n",
    "\n",
    "fig2 = ff.create_annotated_heatmap(z, annotation_text=z, colorscale=colorscale)\n",
    "#fig.update_layout(title_text='Correlation Heat Map')\n",
    "fig2.show()\n",
    "\n",
    "\n",
    "fig2 = ff.create_annotated_heatmap(data, annotation_text=data, colorscale=colorscale)\n",
    "#fig.update_layout(title_text='Correlation Heat Map')\n",
    "fig2.show()\n",
    "\n",
    "\n",
    "z_1d=sum(z,[])\n",
    "data_1d=sum(data,[])\n",
    "print(z_1d)\n",
    "print(data_1d)\n",
    "print(z)\n",
    "print(data)\n",
    "fig = px.histogram(z_1d,range_x=[0, 1])\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(data_1d,range_x=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b8d21",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/discrete-colour-scale-in-plotly-python-26f2d6e21c77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fd946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=corr_val.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7785fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n = np.shape(data)\n",
    "R,C = np.mgrid[:m,:n]\n",
    "out = np.column_stack((C.ravel(),R.ravel(),sum(data, [])))\n",
    "\n",
    "\n",
    "data1=pd.DataFrame(out)\n",
    "data1.columns=['X','Y','Z']\n",
    "quantiles=np.quantile(data1['Z'],np.arange(0,1,0.1))\n",
    "data1['Z']=data1['Z'].round(3)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7425a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.sort_values(by=['Z'],inplace=True)\n",
    "data1['Order'] = np.arange(len(data1))\n",
    "data1['Order_Range']=round((data1['Order']-data1['Order'].min())/(data1['Order'].max()-data1['Order'].min()),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e17711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data1['Color']=''\n",
    "data1['Correlation']=''\n",
    "data1['Direction']=''\n",
    "data1['Color'][(abs(data1['Z'])<=1) & (abs(data1['Z'])>0.8)]='#8B0000'\n",
    "data1['Correlation'][(abs(data1['Z'])<=1) & (abs(data1['Z'])>0.8)]='High'\n",
    "data1['Color'][(abs(data1['Z'])<=0.8) & (abs(data1['Z'])>0.4)]='#FA8072'\n",
    "data1['Correlation'][(abs(data1['Z'])<=0.8) & (abs(data1['Z'])>0.4)]='High'\n",
    "data1['Color'][(abs(data1['Z'])<=0.4) & (abs(data1['Z'])>0.2)]='#808000'\n",
    "data1['Correlation'][(abs(data1['Z'])<=0.4) & (abs(data1['Z'])>0.2)]='Med'\n",
    "data1['Color'][(abs(data1['Z'])<=0.2) & (abs(data1['Z'])>=0)]='#228B22'\n",
    "data1['Correlation'][(abs(data1['Z'])<=0.2) & (abs(data1['Z'])>=0)]='Low'\n",
    "data1['Direction'][data1['Z']<0]='Neg'\n",
    "data1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85a84a64",
   "metadata": {},
   "source": [
    "agg_ = data1.groupby(['Color','Direction']).agg({'Z': [ 'min', 'max']})\n",
    "temp=agg_.reset_index()[['Color','Direction']].join(agg_.reset_index()['Z'])\n",
    "temp.columns=['Color','Direction','min','max']\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a52f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_ = data1.groupby(['Color','Direction']).agg({'Order_Range': [ 'min', 'max']})\n",
    "temp=agg_.reset_index()[['Color','Direction']].join(agg_.reset_index()['Order_Range'])\n",
    "temp.columns=['Color','Direction','min','max']\n",
    "temp.sort_values(['min'],inplace=True)\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale=[]\n",
    "for i in range(len(temp)):\n",
    "    colorscale.append((round(temp['min'].iloc[i],3),temp['Color'].iloc[i]))\n",
    "    colorscale.append((round(temp['max'].iloc[i],3),temp['Color'].iloc[i]))\n",
    "    #print(temp['Color'].iloc[i])\n",
    "    #print(temp['max'].iloc[i])\n",
    "    #print(temp['min'].iloc[i])\n",
    "    #print (colorscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fb2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee56d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.sort_values(['Y','X'],inplace=True)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0760d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_ = list(data1['Z'])\n",
    "corr_val_=np.array(list_).reshape(int(data1['X'].max()+1),int(data1['Y'].max()+1))\n",
    "list_ = list(data1['Order_Range'])\n",
    "color_val_=np.array(list_).reshape(int(data1['X'].max()+1),int(data1['Y'].max()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12dd757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig2 = ff.create_annotated_heatmap(color_val_, x=x_names,y=y_names, annotation_text=corr_val_ , colorscale=colorscale)\n",
    "fig2.update_layout(title_text='Correlation Heat Map')\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cramers=corr_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac3bab4",
   "metadata": {},
   "source": [
    "Asymetric correlation - Theil's U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb173e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "def conditional_entropy(x,y):\n",
    "    # entropy of x given y\n",
    "    y_counter = Counter(y)\n",
    "    xy_counter = Counter(list(zip(x,y)))\n",
    "    total_occurrences = sum(y_counter.values())\n",
    "    entropy = 0\n",
    "    for xy in xy_counter.keys():\n",
    "        p_xy = xy_counter[xy] / total_occurrences\n",
    "        p_y = y_counter[xy[1]] / total_occurrences\n",
    "        entropy += p_xy * math.log(p_y/p_xy)\n",
    "    return entropy\n",
    "\n",
    "def theils_u(x, y):\n",
    "    s_xy = conditional_entropy(x,y)\n",
    "    x_counter = Counter(x)\n",
    "    total_occurrences = sum(x_counter.values())\n",
    "    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n",
    "    s_x = ss.entropy(p_x)\n",
    "    if s_x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return (s_x - s_xy) / s_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_list=list(df_coded.columns)\n",
    "len(corr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4970fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cat=corr\n",
    "for i in range(len(corr)):\n",
    "    for j in range(len(corr)):\n",
    "        corr_cat.iloc[i,j]=theils_u(df_coded[corr_list[i]],df_coded[corr_list[j]])\n",
    "        \n",
    "corr_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc356b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "corr_val=corr_cat.values\n",
    "x_names=list(corr_cat.columns)\n",
    "y_names=list(corr_cat.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=corr_val.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n = np.shape(data)\n",
    "R,C = np.mgrid[:m,:n]\n",
    "out = np.column_stack((C.ravel(),R.ravel(),sum(data, [])))\n",
    "\n",
    "\n",
    "data1=pd.DataFrame(out)\n",
    "data1.columns=['X','Y','Z']\n",
    "quantiles=np.quantile(data1['Z'],np.arange(0,1,0.1))\n",
    "data1['Z']=data1['Z'].round(3)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.sort_values(by=['Z'],inplace=True)\n",
    "data1['Order'] = np.arange(len(data1))\n",
    "data1['Order_Range']=round((data1['Order']-data1['Order'].min())/(data1['Order'].max()-data1['Order'].min()),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ef974",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['Color']=''\n",
    "data1['Correlation']=''\n",
    "data1['Direction']=''\n",
    "data1['Color'][(abs(data1['Z'])<=1) & (abs(data1['Z'])>0.8)]='#8B0000'\n",
    "data1['Correlation'][(abs(data1['Z'])<=1) & (abs(data1['Z'])>0.8)]='High'\n",
    "data1['Color'][(abs(data1['Z'])<=0.8) & (abs(data1['Z'])>0.4)]='#FA8072'\n",
    "data1['Correlation'][(abs(data1['Z'])<=0.8) & (abs(data1['Z'])>0.4)]='High'\n",
    "data1['Color'][(abs(data1['Z'])<=0.4) & (abs(data1['Z'])>0.2)]='#808000'\n",
    "data1['Correlation'][(abs(data1['Z'])<=0.4) & (abs(data1['Z'])>0.2)]='Med'\n",
    "data1['Color'][(abs(data1['Z'])<=0.2) & (abs(data1['Z'])>=0)]='#228B22'\n",
    "data1['Correlation'][(abs(data1['Z'])<=0.2) & (abs(data1['Z'])>=0)]='Low'\n",
    "data1['Direction'][data1['Z']<0]='Neg'\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_ = data1.groupby(['Color','Direction']).agg({'Order_Range': [ 'min', 'max']})\n",
    "temp=agg_.reset_index()[['Color','Direction']].join(agg_.reset_index()['Order_Range'])\n",
    "temp.columns=['Color','Direction','min','max']\n",
    "temp.sort_values(['min'],inplace=True)\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fb048",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale=[]\n",
    "for i in range(len(temp)):\n",
    "    colorscale.append((round(temp['min'].iloc[i],3),temp['Color'].iloc[i]))\n",
    "    colorscale.append((round(temp['max'].iloc[i],3),temp['Color'].iloc[i]))\n",
    "    #print(temp['Color'].iloc[i])\n",
    "    #print(temp['max'].iloc[i])\n",
    "    #print(temp['min'].iloc[i])\n",
    "    #print (colorscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.sort_values(['Y','X'],inplace=True)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11955715",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = list(data1['Z'])\n",
    "corr_val_=np.array(list_).reshape(int(data1['X'].max()+1),int(data1['Y'].max()+1))\n",
    "list_ = list(data1['Order_Range'])\n",
    "color_val_=np.array(list_).reshape(int(data1['X'].max()+1),int(data1['Y'].max()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = ff.create_annotated_heatmap(color_val_, x=x_names,y=y_names, annotation_text=corr_val_ , colorscale=colorscale)\n",
    "fig2.update_layout(title_text='Correlation Heat Map')\n",
    "fig2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
